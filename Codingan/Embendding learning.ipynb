{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de4oxp0FaQV4"
      },
      "source": [
        "References - https://github.com/openai/openai-cookbook/blob/main/examples/third_party/financial_document_analysis_with_llamaindex.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import ServiceContext\n",
        "from llama_index.core.readers.json import JSONReader\n",
        "from llama_index.core import Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# os.environ['OPENAI_API_KEY'] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "model_path = '../Model/tinylmma-1b'\n",
        "model_name_litteral = \"tinylmma-1b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Frederick\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Frederick\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Frederick\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Download and save the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda')\n",
        "model.save_pretrained(model_path)\n",
        "\n",
        "# Download and save the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zMzJLGLshm2"
      },
      "source": [
        "Load Documents for Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ4-QIeGWz1g",
        "outputId": "7bc3be89-df4b-41bf-86db-c35aab0ee252"
      },
      "outputs": [],
      "source": [
        "docs = SimpleDirectoryReader('../Summarizer result/').load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2qzEmW-uooj"
      },
      "source": [
        "Service Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8kzBZ29uJoc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Frederick\\AppData\\Local\\Temp\\ipykernel_31040\\787780752.py:2: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(llm=llm)\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(temperature=1, model=\"gpt-3.5-turbo-16k\")\n",
        "service_context = ServiceContext.from_defaults(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vectoring Dataset and Save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Model for embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name TinyLlama/TinyLlama-1.1B-Chat-v1.0. Creating a new one with MEAN pooling.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=model_name, \n",
        "    device=\"cuda\",\n",
        "    max_length=512\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llz_Jp4-dQzK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Frederick\\anaconda3\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:728: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        }
      ],
      "source": [
        "Settings.embed_model = embed_model\n",
        "index = VectorStoreIndex.from_documents(docs)  \n",
        "index.storage_context.persist(persist_dir=\"../VectorizedData/{model_name_litteral}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Dataset (VectorDB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K0moJJJyHthd"
      },
      "outputs": [],
      "source": [
        "Settings.embed_model = embed_model\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"../VectorizedData/{model_name_litteral}/\") #fill with dataset that has been saved\n",
        "index = load_index_from_storage(storage_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9OqAaxzs0Vi"
      },
      "source": [
        "Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model `TinyLlama/TinyLlama-1.1B-Chat-v1.0` and tokenizer `StabilityAI/stablelm-tuned-alpha-3b` are different, please ensure that they are compatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask_token_id: None\n",
            "sep_token_id: None\n",
            "pad_token_id: 2\n",
            "eos_token_id: 2\n",
            "cls_token_id: None\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
        "from llama_index.core import Settings\n",
        "\n",
        "system_prompt = \"\"\"# TinyLlama Game Recommendation Model\n",
        "The TinyLlama Game Recommendation Model is an innovative language model created by the TinyLlama project, aimed at enhancing game recommendation systems within the Steam platform.\n",
        "This model excels in understanding and generating diverse linguistic inputs, empowering it to provide highly personalized game recommendations.\n",
        "Built on a sophisticated transformer architecture, the TinyLlama Game Recommendation Model is optimized to suggest games that align closely with individual user preferences and gaming behaviors, elevating the gaming journey for Steam enthusiasts.\n",
        "\"\"\"\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"{query_str}\")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=1450, \n",
        "    max_new_tokens=400,\n",
        "    system_prompt=system_prompt,\n",
        "    generate_kwargs={\"temperature\": 0.2, \"do_sample\": True},\n",
        "    model_name=model_name,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    tokenizer_kwargs={\"truncation\": True},\n",
        "    model_kwargs={\"torch_dtype\": torch.float32, \"pad_token_id\": tokenizer.pad_token_id, \"device\": \"cuda\"},\n",
        ")\n",
        "\n",
        "print(\"mask_token_id:\", tokenizer.mask_token_id)\n",
        "print(\"sep_token_id:\", tokenizer.sep_token_id)\n",
        "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
        "print(\"eos_token_id:\", tokenizer.eos_token_id)\n",
        "print(\"cls_token_id:\", tokenizer.cls_token_id)\n",
        "query_engine = index.as_query_engine(similarity_top_k=3, llm=llm)\n",
        "response = query_engine.query(\"What do players do in 'Stardew Valley'?\")\n",
        "print(\"Response overall:\")\n",
        "print(response)\n",
        "for node in response.source_nodes:\n",
        "    file_name = node.node.metadata['file_name']\n",
        "    node_id = node.node.id_\n",
        "    node_score = node.score\n",
        "    print(f\"File ID: {file_name}, Node ID: {node_id},Score: {node_score},\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
